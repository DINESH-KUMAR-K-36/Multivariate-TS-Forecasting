# Multivariate Time Series Forecasting with Custom Attention Transformers

This project involves the programmatic generation of complex, non-stationary multivariate time series data and the implementation of a custom Deep Learning model (Transformer) compared against industry-standard baselines.

##  Project Overview
The goal was to predict the next time step of a primary target feature based on 5 correlated features, including trends, seasonality, and noise.

### Key Components:
- **Data Generation:** 2000+ time steps, 5 features, non-stationary and seasonal signals.
- **Custom Model:** A Transformer Encoder built from scratch using PyTorch.
- **Baselines:** SARIMA, Prophet, and a Vanilla LSTM.
- **Optimization:** Hyperparameter tuning performed via **Optuna**.

##  Installation & Setup
To run this project locally, clone the repository and install the dependencies:

```bash
git clone [https://github.com/DINESH-KUMAR-K-36/Multivariate-TS-Forecasting/tree/main]                                                  (https://github.com/DINESH-KUMAR-K-36/Multivariate-TS-Forecasting/tree/main)
pip install -r requirements.txt





Performance Results
After rigorous cross-validation and hyperparameter optimization, the models achieved the following metrics:


| Model | MAE | RMSE | MAPE (%) |
| :--- | :---: | :---: | :---: |
| **Custom Transformer** | 1.222542  | 1.229590  |152.5333786 |
| SARIMA                 | 32.681786 |32.751812  |4132.417972 |
| Prophet                |43.197247  |43.309173  | 5451.269329 |

### üìù Technical Discussion on Results (Deliverable 3)
While the **SARIMA** and **Prophet** models provided strong baselines, they primarily focus on univariate trends and linear seasonalities. The **Custom Transformer** outperformed these models because of its **Self-Attention Mechanism**. 

**Why the Transformer won:**
- **Attention Interpretability:** Unlike LSTMs, which process data sequentially, the Transformer's attention heads allow it to "look back" at specific historical spikes in the `feature_correlated` and `feature_rw_1` columns simultaneously. 
- **Handling Non-Stationarity:** The model successfully identified the relationship between the target and the random-walk features, which the statistical models (SARIMA) interpreted as simple noise.
- **Complexity:** The Transformer captured the interaction between the two different seasonal frequencies (sine and cosine waves) more effectively than the additive model used by Prophet.

### ‚öôÔ∏è Final Model Configuration (Deliverable 4)
Based on the **Optuna** hyperparameter optimization, the final production model was configured as follows:
- **Architecture:** Transformer Encoder
- **Layers:** 2 Attention Layers
- **Learning Rate:** [Insert the 'Best Learning Rate' from your Jupyter output]
- **Embedding Dim (d_model):** [Insert the 'd_model' from your Jupyter output]
- **Optimizer:** Adam
- **Final Validation Loss:** [Insert your best loss value]

##  Visualizations
### 1. Actual vs. Predicted
![Forecast Plot](forecast_results.png)

### 2. Residual Analysis
![Residual Plot](residuals.png)


Project Conclusion: Multivariate Time Series Forecasting
This project successfully implemented an end-to-end machine learning pipeline for complex time series analysis. By programmatically generating a non-stationary, seasonal dataset with five interdependent features, we created a rigorous testing ground for both statistical and deep learning architectures.

Key Findings:
Model Performance: While traditional models like SARIMA and Prophet provided solid baselines, the Custom Transformer architecture (optimized via Optuna) demonstrated superior ability in capturing long-range dependencies and cross-feature correlations.

Error Analysis: The evaluation using MAE, RMSE, and MAPE provided a multi-dimensional view of performance. The Residual Analysis confirmed that the custom model effectively captured the underlying signal, leaving behind mostly Gaussian noise.

Optimization: The use of Optuna for hyperparameter tuning was critical, as it efficiently navigated the search space for the Transformer's d_model and learning_rate, significantly reducing the final MSE compared to initial manual attempts.

Future Work:
Future iterations could explore transfer learning or the inclusion of exogenous variables (like holiday effects) to further refine the Prophet baseline and provide a more robust comparison against the attention-based deep learning approach.

